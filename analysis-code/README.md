# How to replicate the experiments.

## Set up the corpus.

First get a copy of the programs from the Qualitas suite.
Create a directory to put them in - let's call it QDIR.
Create QDIR/qualitas, and run the script get-qualitas.sh in there.
This clones the repos and does some renaming to tidy things up.
You should end up with 51 directories and about 4.8G of files.

Edit qualitas.py, and change the variable _CORPUS_ROOT to be your
QDIR.  If you now run qualitas.py it should print out your directory:

     python3 qualitas.py

This is how the remaining shell scripts figure out where things are.

## Get the 'old' year-by-year versions of the applications.

For our longitudinal study we get version of the corpus at 31 Dec from
2005 to 2017. Run the file [rollback.py](./rollback.py) and save the
output somewhere, e.g.:

     python3 rollback.py > roller.sh

This takes a while as it has to go through the repos and find the
relevant versions for each year and each app.

Now create a folder for these older versions as: QDIR/older-versions
Go here and run roller.sh.  This will copy each app from your original
version and roll it back to the relevant version for each year.
Have a look at roller.sh: it basically has the SHAs for the versions
you want; the rest is just copying and a git reset.


## Data for table 1: basic counts, commits

The basic information is derived from running
[list-details.sh](./list-details.sh).
This just uses 'find' and 'wc' to get file and line counts, and uses
'git' to get the first commit.

Getting the last commit was a little more troublesome, since the apps
do not always tag things consistently.  To deal with this, I run
list-recent-tags.sh to get the most recent 20 tags, and then manually
pick out the most suitable-looking one.  In most cases this is the
first one, but some apps are a bit more awkward.


## Data for table 2: pass rates (running PyComply).

In order to build and run the PyComply front-end (and assuming you
have flex, bison and gcc installed) you want to run
[qualitas-test.py](./qualitas-test.py).

For example, to run the v2.7 of PyComply over the astropy application:

		python3 qualitas_test.py 2.7 astropy

aThe first time you do this, you'll see it building the 2.7 PyComply;
after this it just uses the built version.  The output of the analysis
is a latex table with the pass rate(s) for each application (rows) and
PyComply version (columns).

You can try multiple versions and applications, e.g.

    python3 qualitas_test.py 2.7 astropy 3.1 gramps

will run both 2.7 and 3.1 PyComply on these two apps.

The actual source code for PyComply is in the
[multi-front-end](./multi-front-end)
folder.
The Python script just runs make here, which then gathers the correct
lexer and parser source files, and builds the relevant PyComply version.

The pass rates for the _older_ versions of the applications (2016,
2015, ...) are generated by
[historical_comply.py](./historical_comply.py)
(note that this
script will also plot some figures).  This just tweaks some paths
etc. and then runs qualitas_test.py.

If you want to bypass the generation of front-ends and pass rates, the
pass rates for each Python front-end and each version of the Qualitas
suite are given in the files in

    [../archived-data/pass_rates/](../archived-data/pass_rates/)

For example,
[qualitas-2010-12-31-3.2.dat](../archived-data/pass_rates/qualitas-2010-12-31-3.2.dat)
has the pass rates using
the 2010 version of the apps and the Python 3.2 front-end.


## Data for figure 3: activity for (dates of) Python 3 versions

The script
[count-commit-dates.sh](./count-commit-dates.sh)
gets the commit information from the
git logs for the latest version of each app.  It then runs a loop to
categorise these into the various date ranges corresponding to the
Python releases, which it gets by running python_versions.py.

This data is written to
[commit-data.csv](../archived-data/commit-data.csv),
which shows the
number of commits for each Python 3.x version from 3.6 down to 3.0,
(NB: most recent first)
and then the total no. of commits at the end.

For example, in the paper we give the percentages for calibre; the
actual numbers behind this are:

     >grep calibre ../archived-data/commit-data.csv 
      calibre   2651   5473   9247  14502  22794  32118  34175  36453

The violin plot is created by the script
[plot_commits.py](./plot_commits.py)
which reads this file.


## Data for figures 4 and 5: numbers of committers and creators

The data is collected by the script
[count-authors.sh](./count-authors.sh). 
This is just a matter of grepping the git logs for the relevant
information.

The data is then stored in the file
[authors.txt](../archived-datadata/authors.txt).
For each application
this lists the number of committers and creators.  We also counted the
number of committers with more than 5, 10 etc. commits, but didn't use
this in the paper.

The two box-plots are then generated by the script
[plot_authors.py](./plot_authors.py).


## Data for figures 6 and 7: changes in compliance level.

This is actually produced by historical_comply.py as well.
It will read the pass rate data from the files if they're there,
otherwise it will regenerate the pass rates.  The series (2 or 3) is
hard coded at the end of the file.

## Data for section 6: back-ported features.

This is produced by a specialised version of the 2.7 front-end,
augmented by some counters.  This front-end is in the
[feature-finder](./feature-finder)
directory.  If you run the Python file there with an app name, e.g.

     > python3 find_features.py astropy

It will parse these files using the specialised 2.7 front-end, and
print a summary of the backported 3x features it finds.


### Authors are:

* [Brian A. Malloy](http://www.brianmalloy.com/),
  Clemson University, SC, USA
* [James F. Power](http://www.cs.nuim.ie/~jpower/),
  Maynooth University, Ireland


